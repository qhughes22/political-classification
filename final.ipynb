{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the writeup, the three learners, and the ensemble learner, along with some extra analysis. \n",
    "\n",
    "forest.ipynb includes the code used for a grid search to get the optimal parameters for the random forest. logistic_regression.ipynb includes the code for its respective grid search as well as the code for collecting data. The data collected was saved as the csv fulldatasetwithends.csv. Not all code submitted was run exactly as is. For the grid searches, I submitted code that ran grid searches on only 1% of the dataset, since they took a long time. And I just commented out the calling of the data collection function for the same reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classifying Congressional Tweets Using Various Machine Learning Algorithms**\n",
    "\n",
    "For my project I decided to look at applying machine learning to classify by party tweets made by Congresspeople and Senators. In particular, I wanted to know if ensemble methods combining different learners could lead to better predictions. Data were collected from twitter using the tweepy library, all tweets by people on this list of Congressmen and Senators were used: https://triagecancer.org/congressional-social-media. This source for twitter handles was used as it includes party info. Those who were not Republicans or Democrats were filtered out. \n",
    "\n",
    "Obviously, lots of work has been done in the field of sentiment analysis, even in the specific political classification area. Some studies have found that political orientation classification can be difficult (The Perils of Classifying Political Orientation From Text;\n",
    "Hao Yan, Allen Lavoie, and Sanmay Das; https://www.cse.wustl.edu/~sanmay/papers/political-orientation.pdf) because political language isn’t necessarily constant in different media. I think that the dataset I’m using of just tweets should mitigate that problem, as much of the political messaging across politicians on twitter specifically, with a character limit, probably by necessity is limited to buzzwords and hashtags that should be easy to identify. Other analyses have covered looking at the orientation of users as a whole, not specific tweets (Predicting the Political Alignment of Twitter Users; Michael D. Conover, Bruno Gonc¸alves, Jacob Ratkiewicz, Alessandro Flammini and Filippo Menczer; https://cnets.indiana.edu/wp-content/uploads/conover_prediction_socialcom_pdfexpress_ok_version.pdf). Of course, classifying specific tweets is much harder than classifying users as a whole. The analysis done by Conover et al found that using hashtags had the greatest accuracy, but not all tweets have hashtags so I couldn’t take that approach.\n",
    "\n",
    "The approach I did take was to use the bag of words model for two learners and to leverage the sequential nature of a tweet for the third. I trained three models: a logistic regression model, a random forest model, and an lstm recurrent neural network. The first two were done in scikit-learn, the last in tensorflow. For the first two I vectorized using tfidf and some simple preprocessing (to remove links and such). I ran grid searches on both the vectorizer and the model as a whole for both the random forest and logistic regression. I was surprised to find for both that the optimal n_gram range was only 1 (so no word pairs or triplets). The grid search found stop words helpful for both. Other optimal parameters can be found in forest.ipynb and logistic_regression.ipynb. For the logistic regression, before doing the full grid search I determined a K value for SelectKBest by running a few grid searches on subsamples and picking a number that led to a reasonable training time at little cost to accuracy. The lstm was vectorized by a simple Token Text encoding that padded each vector to be the same length as other vectors in the batch.\n",
    "\n",
    "The lstm included an embedding layer, a bidirectional layer, and two dense layers: one with relu and one with a sigmoid activation function. Much of my code for this learner was adapted from Sebastian Raschka’s Python Machine Learning 3rd Edition. Not much hyperparameter optimization was done for this model as I was really most interested in seeing how it overlaps with the other models, and optimization is very time-intensive, as this learner has very high training time (close to an hour!). \n",
    "\n",
    "The ensemble learner simply took a majority vote of the three other learners. I didn’t use the built in majority vote module in scikit-learn because I had a tensorflow model, so I just summed the predictions on the test set (making sure they weren’t accidentally shuffled to not match). This had an accuracy in the range of the neural network and the logistic regression. \n",
    "\n",
    "Results:\n",
    "**Logistic Regression**\n",
    "\n",
    "Accuracy on test set:\n",
    "\n",
    "0.8361050599507642\n",
    "Training time:\n",
    "\n",
    "38.06299066543579\n",
    "\n",
    "Prediction time (test):\n",
    "\n",
    "5.7797932624816895\n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "Accuracy on test set:\n",
    "\n",
    "0.6471051839257567\n",
    "\n",
    "Training time:\n",
    "\n",
    "157.40676164627075\n",
    "\n",
    "Prediction time (test):\n",
    "\n",
    "69.72026896476746\n",
    "\n",
    "**Recurrent Neural Network**\n",
    "\n",
    "Accuracy on test set:\n",
    "\n",
    "80.83%\n",
    "\n",
    "Training time:\n",
    "\n",
    "3024.5883202552795\n",
    "\n",
    "Prediction time (test):\n",
    "\n",
    "69.72026896476746\n",
    "\n",
    "**Majority Vote**\n",
    "\n",
    "Accuracy on test set:\n",
    "\n",
    "82.62%\n",
    "\n",
    "Majority vote training and prediction times are the sum of component parts. I didn't include numbers here because I thought it would be slightly misleading, as you can parallelize training and prediction and I didn't implement that as I didn't think it was too important for this project and it's arguably outside the scope of this course. As you can see, the logistic regression was the standout model, with the lowest training time and highest test set accuracy. The RNN was close in accuracy, but had an abysmally high training time. The random forest was pretty terrible, with very low test set accuracy. Its accuracy was about equal to the proportion of Democrat tweets in the test dataset (about 66%). It classified almost everything as Democrat, and very few (about 1000 in the run I’m submitting) as Republican. Unfortunately, the poor performance of the random forest means that my ensemble learner suffered as well. \n",
    "\n",
    "I suspect the poor performance of the random forest may be because there are too many features for a random forest to do well. If there’s a few words that are indicative within a tweet, then it’s likely only one or two trees in the forest will have been significantly trained on those words. Those trees would be outvoted by the others, and when there’s no information to go on they’ll vote Democrat since the majority of the dataset is Democrat.\n",
    "\n",
    "Other results can be seen below in the code blocks. Because the random forest was poor, I suspected that I couldn’t really test the question of whether a majority vote improves learners because if the two strong learners tie, the tiebreaker random forest would almost always vote Democrat. 75% of those misclassified by the majority were Republican, supporting this theory. So I looked at those wrongly predicted by the majority vote and looked at the vote scores themselves. A 0 or 3 indicates that all three models were wrong, 1 or 2 that 1 model was correct. I found in 74% of cases, at least one model actually had the right answer. I did the same analysis without the random forest and found that in 48% of cases, one of the two stronger models predicted correctly. This lends strong support to the thesis that ensemble learning can be helpful, as these very different models are incorrect about different test examples. This suggests that if a third strong model is found, a majority vote between the three would yield significant improvement.\n",
    "\n",
    "It’s also the case that 100% accuracy is impossible with this dataset. I doubt that a machine learning algorithm could achieve much greater accuracy than a human. I don’t think there are patterns unrelated to politics that are correlated with party. There’s enough diversity across the aisle that I imagine anything a machine could use to classify would be noticed by an educated human. This includes words and phrases like “the wall” or “our troops” or “minority representation”. Tweets that don’t have political content (like simple happy birthday wishes) are bound to be misclassified. So I grabbed a sample of some of the misclassified tweets to see how hard they are for a person to identify. Many of them I couldn’t really figure out correctly, although some of them I could. An example of the former: “You can also join us by calling 855-859-6912! Press * 3 to ask a question!”. I don’t think any algorithm or person is gonna be more than 50% likely to classify that correctly. An example of the latter: “For those insisting Mail-in Voting doesn’t cause voter fraud; election theft, New Jersey proves otherwise (Note: Absentee Voting verification procedures reduce fraud).” This one is by a Republican, as should be obvious to anyone who’s paid attention this election. So clearly there’s room for improvement if this got misclassified.\n",
    "\n",
    "As I discussed above, there’s quite a bit of disagreement between the two strong models. This means that a third model could be added (assuming it’s better than the random forest) that would lead to a majority voting model that’s significantly better than any of its component parts. I think a support vector machine might be a good candidate, as that’s what Conover et al used. Other improvements can be made on the models themselves. More hyperparameter tuning, particularly on the recurrent neural network, would likely improve performance. Future work could explore these possibilities and improve the models I made here.\n",
    "\n",
    "But overall from this project it is clear that classification of individual tweets by party is viable, and ensemble learning is a viable possibility to improve it. 80% success isn’t bad, and I’ve laid out some clear ways to get even better results. This project may not seem all that relevant on its face, but there’s clear uses. For one thing, a model can be trained on Congresspeople and then generalized and used to predict the leanings of tweets by general Twitter users, or used to predict on any text at all. The work of Yan et al suggests this may not work though, since the training population needs to have similar speech patterns as the testing population. The general twitter population likely writes informally compared to Senators and Congresspeople. Also, with both the logistic regression and the recurrent neural networks, the probability estimates can be used to suggest how partisan different politicians are. It could assess whether a candidate is trying to appeal to the middle ground or trying to appeal to his/her parties base. If a Democrat candidate’s tweets are strongly predicted Democrat, then it would be the latter. If the prediction is weaker, then the former seems true. The results of this project suggest the viability of those options, and they should be further explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prototype writeup:**\n",
    "\n",
    "Title: Classifying Congressional Tweets Using Various Machine Learning Algorithms\n",
    "\n",
    "Description:\n",
    "\n",
    "I am trying to read tweets written by Congresspeople (Senate and House) and use machine learning to predict what party they're from. The tweets are collected using Twitter's API and the python libraryTweepy. I'm converting the text of the words to a bag of words, filtering out punctuation and stemming as well. I'm leaving in hashtags and @s. \n",
    "\n",
    "In terms of specific algorithms, I expect to use multiple different classifiers. I also will use a neural net,though I'm not sure the architecture I'll aim for. I still need to do more research on what's commonly used for NLP. It will likely incorporate existing sentiment analysis models to improve accuracy. In addition to that, I'll try a number of different classifiers, using grid search to optimize them, and test them with both test set accuracy and k-fold cross validation.\n",
    "\n",
    "I plan to present the best models based on multiple metrics, including accuracy, time to learn, and time to classify. I'll include the optimized parameters as well, and the optimal preparatory pipelines.\n",
    "\n",
    "I've done most of the data preprocessing and collected a decent-sized dataset. I've run a logistic\n",
    "regression classifier with default hyperparameters and a tfidf vectorizer. Stemming and filtering functions are completed. You can see the accuracy of this basic model below; with 75% accuracy on the test set, I'd say that's a good proof of concept.\n",
    "\n",
    "I still need to complete my other models, and I also would like to get a larger dataset. I need to see how to get more tweets per person using tweepy. I'll also look into other tools for preprocessing. And I'll look into other dictionaries that could be helpful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\quincy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from joblib import load, dump\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>account</th>\n",
       "      <th>text</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>Today is #SmallBusinessSaturday! I encourage a...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>Wishing everyone a very happy #Thanksgiving. T...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>The @USAirForce has selected Maxwell Air Force...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>Great news! @NSF recently awarded @SamfordU $1...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SenShelby</td>\n",
       "      <td>Thank you to each and every one of the brave m...</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376415</th>\n",
       "      <td>376415</td>\n",
       "      <td>RepGwenMoore</td>\n",
       "      <td>In conjunction with the reckless decision to p...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376416</th>\n",
       "      <td>376416</td>\n",
       "      <td>RepGwenMoore</td>\n",
       "      <td>Without consultation with or authorization fro...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376417</th>\n",
       "      <td>376417</td>\n",
       "      <td>RepGwenMoore</td>\n",
       "      <td>I will continue doing everything in my power t...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376418</th>\n",
       "      <td>376418</td>\n",
       "      <td>RepGwenMoore</td>\n",
       "      <td>Poverty is rising in parts of the US. We need ...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376419</th>\n",
       "      <td>376419</td>\n",
       "      <td>RepGwenMoore</td>\n",
       "      <td>Happy New Year! Hoping that 2020 brings you ha...</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376420 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0       account  \\\n",
       "0                0     SenShelby   \n",
       "1                1     SenShelby   \n",
       "2                2     SenShelby   \n",
       "3                3     SenShelby   \n",
       "4                4     SenShelby   \n",
       "...            ...           ...   \n",
       "376415      376415  RepGwenMoore   \n",
       "376416      376416  RepGwenMoore   \n",
       "376417      376417  RepGwenMoore   \n",
       "376418      376418  RepGwenMoore   \n",
       "376419      376419  RepGwenMoore   \n",
       "\n",
       "                                                     text party  \n",
       "0       Today is #SmallBusinessSaturday! I encourage a...     R  \n",
       "1       Wishing everyone a very happy #Thanksgiving. T...     R  \n",
       "2       The @USAirForce has selected Maxwell Air Force...     R  \n",
       "3       Great news! @NSF recently awarded @SamfordU $1...     R  \n",
       "4       Thank you to each and every one of the brave m...     R  \n",
       "...                                                   ...   ...  \n",
       "376415  In conjunction with the reckless decision to p...     D  \n",
       "376416  Without consultation with or authorization fro...     D  \n",
       "376417  I will continue doing everything in my power t...     D  \n",
       "376418  Poverty is rising in parts of the US. We need ...     D  \n",
       "376419  Happy New Year! Hoping that 2020 brings you ha...     D  \n",
       "\n",
       "[376420 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in the dataset\n",
    "\n",
    "origdf = pd.read_csv('fulldatasetwithends.csv').dropna()\n",
    "origdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process text\n",
    "import string\n",
    "\n",
    "removable_punctuation = \"!\\\"$%&'()*+,-./:;<=>?[\\]^_`{|}~\"\n",
    "translator = str.maketrans('', '', removable_punctuation)\n",
    "\n",
    "def remove_links(text):\n",
    "    index = text.find('https://t.co/')\n",
    "    if index is -1:\n",
    "        return text\n",
    "    else:\n",
    "        text = text[0:index] + text[index+23:]\n",
    "        return remove_links(text)\n",
    " \n",
    "\n",
    "def preprocessor(text):\n",
    "    try:\n",
    "        text = remove_links(text)\n",
    "#         print(text[index+23:])\n",
    "    except:\n",
    "        print(text)\n",
    "    text = text.replace(\"&amp;\", '').lower()\n",
    "    return text.translate(translator)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataframe, convert party to binary (0 = Democrat, 1 = Republican)\n",
    "df = origdf.sample(frac=1)\n",
    "\n",
    "df['party'] =df['party']=='R'\n",
    "df['party'] = df['party'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republicans:\n",
      "135933\n",
      "Democrats:\n",
      "240487\n"
     ]
    }
   ],
   "source": [
    "print(\"Republicans:\")\n",
    "print(sum(df['party']))\n",
    "print(\"Democrats:\")\n",
    "print(len(df) - sum(df['party']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "stop = []\n",
    "\n",
    "#need to put stopwords through preprocessor, since stopwords have punctuation and features don't\n",
    "for word in stopwords.words('english'):\n",
    "    stop.append(preprocessor(word))\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split() if word not in stop]\n",
    "\n",
    "X = list(df['text']) \n",
    "y = list(df['party'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.9276567967392046\n",
      "Accuracy on test set:\n",
      "0.8361050599507642\n",
      "Training time:\n",
      "38.06299066543579\n",
      "Prediction time (test):\n",
      "5.7797932624816895\n"
     ]
    }
   ],
   "source": [
    "#logistic regression with hyperparameters taken from the grid search\n",
    "from joblib import dump\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=preprocessor)\n",
    "\n",
    "\n",
    "\n",
    "lr = Pipeline([('vect', tfidf),('clf', LogisticRegression(random_state=0, solver='liblinear', C=100, penalty= 'l2'))])\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "trainingtime = time.time()-starttime\n",
    "results_train = lr.predict(X_train) == y_train\n",
    "startpredicttime = time.time()\n",
    "results_test = lr.predict(X_test) == y_test\n",
    "predicttime = time.time()-startpredicttime\n",
    "\n",
    "print(\"Accuracy on training set:\")\n",
    "print(np.count_nonzero(results_train)/len(results_train))\n",
    "print(\"Accuracy on test set:\")\n",
    "print(np.count_nonzero(results_test)/len(results_test))\n",
    "print(\"Training time:\")\n",
    "print(trainingtime)\n",
    "print(\"Prediction time (test):\")\n",
    "print(predicttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set:\n",
      "0.6475441566031864\n",
      "Accuracy on test set:\n",
      "0.6471051839257567\n",
      "Training time:\n",
      "157.40676164627075\n",
      "Prediction time (test):\n",
      "69.72026896476746\n"
     ]
    }
   ],
   "source": [
    "# random forest classifier with (some) hyperparameters taken from the grid search\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import time\n",
    "\n",
    "starttime = time.time()\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=preprocessor, tokenizer=tokenizer_porter,\n",
    "                        stop_words=stop)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rf = Pipeline([('vect', tfidf),('clf', RandomForestClassifier(n_estimators=10, random_state=0,\n",
    "                                                             max_depth=20, max_features='auto',\n",
    "                                                             n_jobs=-1))])\n",
    "\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train) \n",
    "trainingtime = time.time()-starttime\n",
    "results_train = rf.predict(X_train) == y_train\n",
    "startpredicttime = time.time()\n",
    "results_test = rf.predict(X_test) == y_test\n",
    "predicttime = time.time()-startpredicttime\n",
    "\n",
    "print(\"Accuracy on training set:\")\n",
    "print(np.count_nonzero(results_train)/len(results_train))\n",
    "print(\"Accuracy on test set:\")\n",
    "print(np.count_nonzero(results_test)/len(results_test))\n",
    "print(\"Training time:\")\n",
    "print(trainingtime)\n",
    "print(\"Prediction time (test):\")\n",
    "print(predicttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hello tomatoes  peppers #covid19garden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>still quite excited that just this past weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when the trump administration relaxes enforcem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>minimizing the risk and severity of this virus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>honored to have fought for montana every step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263489</th>\n",
       "      <td>in the #caresact senate republicans and the wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263490</th>\n",
       "      <td>thanks for always having my back sis @ayannapr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263491</th>\n",
       "      <td>too many servicemembers come home with invisib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263492</th>\n",
       "      <td>a president that values american lives doesnt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263493</th>\n",
       "      <td>pass the heroes act</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263494 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "0                 hello tomatoes  peppers #covid19garden \n",
       "1       still quite excited that just this past weeken...\n",
       "2       when the trump administration relaxes enforcem...\n",
       "3       minimizing the risk and severity of this virus...\n",
       "4       honored to have fought for montana every step ...\n",
       "...                                                   ...\n",
       "263489  in the #caresact senate republicans and the wh...\n",
       "263490  thanks for always having my back sis @ayannapr...\n",
       "263491  too many servicemembers come home with invisib...\n",
       "263492  a president that values american lives doesnt ...\n",
       "263493                               pass the heroes act \n",
       "\n",
       "[263494 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting train and test lists back to dataframes for the RNN\n",
    "\n",
    "rnn_df_train = pd.DataFrame(list(zip(X_train, y_train)), \n",
    "               columns =['text', 'party'])\n",
    "\n",
    "rnn_df_test = pd.DataFrame(list(zip(X_test, y_test)), \n",
    "               columns =['text', 'party'])\n",
    "rnn_df_test_copy = rnn_df_test.copy() #will be used later\n",
    "rnn_df_test['text'] = rnn_df_test['text'].apply(preprocessor)\n",
    "rnn_df_train['text'] = rnn_df_train['text'].apply(preprocessor)\n",
    "\n",
    "\n",
    "target_train = rnn_df_train.pop('party') \n",
    "target_test = rnn_df_test.pop('party') \n",
    "rnn_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensorflow datasets\n",
    "\n",
    "ds_raw_test = tf.data.Dataset.from_tensor_slices((rnn_df_test.values,target_test.values))\n",
    "ds_raw_train_valid =  tf.data.Dataset.from_tensor_slices((rnn_df_train.values,target_train.values))\n",
    "\n",
    "trainsize =(int) (len(ds_raw_train_valid)*0.8)\n",
    "\n",
    "ds_raw_train = ds_raw_train_valid.take(trainsize)\n",
    "\n",
    "ds_raw_valid = ds_raw_train_valid.skip(trainsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing datasets\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "token_counts = Counter()\n",
    "\n",
    "for example in ds_raw_train:\n",
    "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
    "    \n",
    "def encode(text_tensor, label):\n",
    "    text = text_tensor.numpy()[0]\n",
    "    encoded_text = encoder.encode(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "def encode_map_fn(text, label):\n",
    "    return tf.py_function(encode, inp=[text,label], Tout=(tf.int64, tf.int64)) # 64 or 32?\n",
    "\n",
    "ds_train = ds_raw_train.map(encode_map_fn)\n",
    "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
    "ds_test = ds_raw_test.map(encode_map_fn)\n",
    "\n",
    "    \n",
    "train_data = ds_train.padded_batch(32, padded_shapes=([-1],[]))\n",
    "valid_data = ds_valid.padded_batch(32, padded_shapes=([-1],[]))\n",
    "test_data = ds_test.padded_batch(32, padded_shapes=([-1],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed-layer (Embedding)      (None, None, 20)          2134520   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               43520     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 2,186,361\n",
      "Trainable params: 2,186,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "6588/6588 [==============================] - 302s 46ms/step - loss: 0.4058 - accuracy: 0.8081 - val_loss: 0.3564 - val_accuracy: 0.8363\n",
      "Epoch 2/10\n",
      "6588/6588 [==============================] - 296s 45ms/step - loss: 0.3038 - accuracy: 0.8642 - val_loss: 0.3557 - val_accuracy: 0.8406\n",
      "Epoch 3/10\n",
      "6588/6588 [==============================] - 290s 44ms/step - loss: 0.2520 - accuracy: 0.8898 - val_loss: 0.3827 - val_accuracy: 0.8370\n",
      "Epoch 4/10\n",
      "6588/6588 [==============================] - 294s 45ms/step - loss: 0.2083 - accuracy: 0.9099 - val_loss: 0.4208 - val_accuracy: 0.8317\n",
      "Epoch 5/10\n",
      "6588/6588 [==============================] - 293s 44ms/step - loss: 0.1683 - accuracy: 0.9290 - val_loss: 0.4836 - val_accuracy: 0.8242\n",
      "Epoch 6/10\n",
      "6588/6588 [==============================] - 299s 45ms/step - loss: 0.1352 - accuracy: 0.9441 - val_loss: 0.5509 - val_accuracy: 0.8215\n",
      "Epoch 7/10\n",
      "6588/6588 [==============================] - 325s 49ms/step - loss: 0.1098 - accuracy: 0.9549 - val_loss: 0.5952 - val_accuracy: 0.8176\n",
      "Epoch 8/10\n",
      "6588/6588 [==============================] - 305s 46ms/step - loss: 0.0915 - accuracy: 0.9625 - val_loss: 0.6473 - val_accuracy: 0.8129\n",
      "Epoch 9/10\n",
      "6588/6588 [==============================] - 301s 46ms/step - loss: 0.0769 - accuracy: 0.9686 - val_loss: 0.7246 - val_accuracy: 0.8058\n",
      "Epoch 10/10\n",
      "6588/6588 [==============================] - 317s 48ms/step - loss: 0.0628 - accuracy: 0.9744 - val_loss: 0.8557 - val_accuracy: 0.8074\n",
      "3529/3529 [==============================] - 51s 15ms/step - loss: 0.8532 - accuracy: 0.80830s - loss: 0\n",
      "Test Accuracy: 80.83%\n",
      "Training time:\n",
      "3024.5883202552795\n",
      "Prediction time (test):\n",
      "69.72026896476746\n"
     ]
    }
   ],
   "source": [
    "#create and train RNN\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Embedding(input_dim=100, output_dim = 6, input_length = 20, name= 'embed-layer'))\n",
    "\n",
    "embedding_dim = 20\n",
    "vocab_size = len(token_counts) + 2\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "rnn = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim = vocab_size,\n",
    "                             output_dim = embedding_dim,\n",
    "                             name = 'embed-layer'),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, name='lstm-kayer')),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "print(rnn.summary())\n",
    "\n",
    "rnn.compile(optimizer = tf.keras.optimizers.Adam(1e-3),\n",
    "                     loss = tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                     metrics=['accuracy'])\n",
    "starttime = time.time()\n",
    "history = rnn.fit(train_data, validation_data = valid_data, epochs=10)\n",
    "trainingtime = time.time()-starttime\n",
    "test_results = rnn.evaluate(test_data)\n",
    "testtime = time.time()-trainingtime\n",
    "print('Test Accuracy: {:.2f}%'.format(test_results[1]*100))\n",
    "\n",
    "print(\"Training time:\")\n",
    "print(trainingtime)\n",
    "print(\"Prediction time (test):\")\n",
    "print(predicttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_predictions = np.concatenate(np.round(rnn.predict(test_data))).astype(int)\n",
    "rnn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is to verify that the test set isn't shuffled for the RNN compared to the others\n",
    "\n",
    "targets = np.concatenate([y for x, y in test_data], axis=0)\n",
    "np.all(targets==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_predictions = lr.predict(X_test)\n",
    "lr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions = rf.predict(X_test)\n",
    "rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1045"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many did rf predict as republican\n",
    "\n",
    "sum(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96543"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(rnn_predictions))\n",
    "sum(rnn_predictions == lr_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, ..., 2, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_predictions = np.add(lr_predictions, rf_predictions)\n",
    "sum_predictions = np.add(sum_predictions, rnn_predictions)\n",
    "sum_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "majority_predictions = sum_predictions>1\n",
    "majority_predictions = majority_predictions.astype(int)\n",
    "majority_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority vote test set accuracy:\n",
      "82.6249048049165\n"
     ]
    }
   ],
   "source": [
    "print(\"Majority vote test set accuracy:\")\n",
    "print(sum(majority_predictions == y_test)/len (y_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text  party\n",
      "11      I have full confidence in @POTUS and @VP's abi...      1\n",
      "18      Thanks to @POTUS and the #CARESAct, Virginia’s...      1\n",
      "20      As we continue to work to safely reopen our co...      1\n",
      "25      I’m live with @scrowder. Tune in here: https:/...      1\n",
      "29      Grateful for the overwhelming support from Ohi...      1\n",
      "...                                                   ...    ...\n",
      "112881  \"Everything\" including court packing on the ag...      1\n",
      "112886  Happy Birthday @RodneyChilders4 ! 🏁🏁🏁🎊🎂🎈🎁🎉👍😎🏁🏁...      1\n",
      "112895  My Clean Cities Bill and EJ for All Act are in...      1\n",
      "112913  Democrats would rather score political points ...      1\n",
      "112915  Trump declared surrender to COVID the minute h...      0\n",
      "\n",
      "[19621 rows x 2 columns]\n",
      "Party: 0 Text: Looking foward to a news conference with @SenMikeLee today at 2pm, to celebrate the 5th Annual Flavors of Utah. Some of Utah's food producers are donating signature Utah  items to local hospitals and first responders. #utpol \n",
      "@ZionsBank \n",
      "@associated_food \n",
      "@utahfood\n",
      "Party: 1 Text: #Coronavirus is a serious risk worldwide. We must protect our service members abroad and make sure the virus doesn't come home with our soldiers. \n",
      " \n",
      "Watch my exchange with @EsperDoD on how we're preventing the transmission of this virus back home👇 https://t.co/TdI6dUpbrG\n",
      "Party: 1 Text: His stories of the sacrifices, heroism &amp; dreams of the Americans he recognized at his speech stirred our hearts, brought tears to our eyes &amp; made us immensely proud.\n",
      "Party: 0 Text: Sending your child to a quality school should not be an opportunity afforded only to some families. \n",
      "\n",
      "School choice is about giving all families the opportunity to choose the best school for their child. \n",
      "\n",
      "#SchoolChoice #SchoolChoiceWeek https://t.co/0yEBsPhqjo\n",
      "Party: 1 Text: In April, I wrote a letter to HHS Secretary Alex Azar requesting allocations of funds for nursing homes to ensure they have the resources they need to protect our seniors and defeat COVID-19.  https://t.co/PPgJarXDe3\n",
      "Party: 0 Text: Thank you to the Islamic Outreach Center of CO &amp; Abu Bakr Mosque for welcoming me yesterday.\n",
      "\n",
      "As the Muslim &amp; Senegalese communities continue to mourn the Diol family, I send my deepest condolences &amp; stand with you in search of justice for Djibril, Adja, Khadija, Hassan &amp; Hawa. https://t.co/1acqKbU3Bf\n",
      "Party: 1 Text: Today is #NationalBookLoversDay! What are you reading, Arizona? https://t.co/G7S1r0BoLm\n",
      "Party: 1 Text: All Americans should give thanks for #FreePress protections that are not common around the world. Praying for @quenqiushi404, all affected by #coronavirus, and swift effective safeguards. https://t.co/IhuFcy6r8D\n",
      "Party: 1 Text: “Once it gets done, it can be something we can be very proud of,” [Boozman]said. “We are going to work as hard as it takes to get the rest of this done.”\n",
      "\n",
      "Read about the latest on my Butterfield Overland Trail legislation via @rsvlcourier:\n",
      "\n",
      "https://t.co/5Zutzp8uBL\n",
      "Party: 0 Text: Help #StopTheSpreadFC now! We are #InThisTogether! https://t.co/PbC7AHDjQm\n",
      "Party: 1 Text: They work hard, every day. They check levees, feed and care for livestock, and make sure there is ample supply for everyone to have what they need. Thank you! https://t.co/MnNv8etMGw\n",
      "Party: 1 Text: 🚨This Thursday, May 21, the @CTXFoodBank will be distributing food at Del Valle High School from 1:00 PM to 4:00 PM. 🚨 \n",
      "\n",
      "More info: https://t.co/sTOB71BdbJ\n",
      "Party: 1 Text: Why are Western journalists letting Iranian officials spew disinformation instead of asking hard questions like: Why did your regime murder 1500 of your own unarmed citizens who were peacefully protesting? Why did your regime use Kataib Hezbollah to attack the US Embassy in Iraq?\n",
      "Party: 1 Text: @CDCgov If it passes the Senate and is signed into law, this program could go a long way toward assisting these truly underserved communities and improving the health and well-being of our fellow citizens.\n",
      "Party: 1 Text: Today, on the 76th anniversary of #DDay, we honor the allied troops who stormed the beaches of Normandy and changed the course of history. The world is forever grateful for their courage. https://t.co/OesIViUibG\n",
      "Party: 1 Text: ‼️ we are down 9 days left for you to complete your 2020 Census. Completing your Census is your constitutional duty. Stop what you're doing, go to https://t.co/Y9qIWgtb1b and complete your Census right now. It only takes 10 minutes. https://t.co/KWG7RYLRcH\n",
      "Party: 1 Text: Read more in my latest press release: https://t.co/krtCx8PJex. https://t.co/XuKs6fQzqj\n",
      "Party: 1 Text: Medical research is lifesaving, life-changing, and cost saving. By providing needed resources to medical researchers, substantial progress has been made in a number of treatments and cures. This is a priority I’ll continue to fight for. https://t.co/egDI4BGH7i\n",
      "Party: 1 Text: THREAD: This is another blatant attempt by Governor Cuomo to sidestep an ounce of accountability. https://t.co/mcpPUgFzOb\n",
      "Party: 1 Text: Dr. Tim Loula’s diagram shows the paths necessary to establish “herd immunity” to #COVID-19 in our workforce. Our veterinarians, after all, developed the concept in the first place. https://t.co/Bx6zARxa3H https://t.co/khh6Tnz58r\n",
      "Party: 1 Text: Next Friday, my office will host a Virtual Service Academy Day. If you’re interested in attending one of our Nation’s Service Academies, I encourage you to participate to receive information about the application process. https://t.co/cW8vTxlARL\n",
      "Party: 1 Text: After today’s Senate hearing featuring Postmaster General DeJoy, it is evident that his plan for the agency would only lead to a more efficient and financially solvent @USPS that is better equipped to deliver for the American people.\n",
      "\n",
      "https://t.co/IXBo7Fctfj\n",
      "Party: 1 Text: Correct. https://t.co/Kmjv3j0g2K\n",
      "Party: 1 Text: I've #HR7149 to allow for temporary protection of 2020 FSA funds in light of the pandemic. People shouldn't be penalized for planning ahead because of unusual circumstances. See my full statement and more details about the bill here:\n",
      "https://t.co/8g33fB569Y\n",
      "Party: 1 Text: You can also join us by calling 855-859-6912! Press * 3 to ask a question!\n",
      "Party: 1 Text: @berthyman Yes we do.  You might notice some Japanese Beetle damage on that tree as well. They weren’t too bad this year. They also get on my wife’s roses.\n",
      "Party: 1 Text: I know many folks have questions about Coronavirus and what they need to know.  That is why I have created this resource for everyone to find answers to their questions.  I hope this can be helpful:\n",
      "https://t.co/oeBXs4G1NJ\n",
      "Party: 0 Text: Thank you @CQnow for this great interview. https://t.co/tJi8KSOLhA\n",
      "Party: 1 Text: ICYMI: This week, I introduced the Cattle Market Transparency Act of 2020. This legislation would bring much-needed transparency and predictability to the cattle market. @MeatPoultry has more information on my bill. https://t.co/vhTdbVZr72\n",
      "Party: 1 Text: As Congress continues to work in a bipartisan, bicameral manner to support small businesses, I am heartened that we moved quickly to improve this program that has already helped millions of small business owners and their workers.\n",
      "Party: 1 Text: For those insisting Mail-in Voting doesn’t cause voter fraud &amp; election theft, New Jersey proves otherwise (Note: Absentee Voting verification procedures reduce fraud).\n",
      "\n",
      "https://t.co/DMEHjbsxFd\n",
      "\n",
      "ARRESTED #Socialist #Democrat public officials &amp; vols worked HARD to steal elections.\n",
      "Party: 0 Text: He recently inked so-called “peace deals” between the United Arab Emirates (UAE), Bahrain, Sudan, and Israel. The only problem? They weren’t peace deals. They’re arms sales to human rights abusers, designed to empower the Gulf States and increase the risk of war with Iran.\n",
      "Party: 1 Text: The Fatal Conceit is believing that any group of experts can acquire knowledge that equates with the dispersed knowledge of millions of individual interactions in an economy or in public health.\n",
      "https://t.co/DsGylYIuIN\n",
      "Party: 1 Text: People must feel secure in their health so they can return to work and our economy can recover. Widespread testing &amp; a vaccine are the most important factors in how we can safely get our lives back to normal. #KSTownhall\n",
      "Party: 0 Text: Thank you! @ASAAL08 https://t.co/rGluZcBEdF\n",
      "Party: 1 Text: Communities across the country are rightfully shaken and the grief felt by many is real. Our own community knows this feeling too well following the death of Mr. Ahmaud Arbery. But, in the wake of Mr. Arbery's death, our community came together to protest peacefully.\n",
      "Party: 1 Text: Join me to show support for our front line heroes in the coronavirus war: the nurses, doctors, first responders, and all of our health care workers. Share your own video of support encouraging them to #StayStrong. https://t.co/VeG3WlVRZQ\n",
      "Party: 0 Text: The Administration continues to block oversight of the Paycheck Protection Program. Americans deserve full transparency in PPP to ensure small businesses are treated equally and taxpayer dollars are spent responsibly. https://t.co/mUxWjxrCUw\n",
      "Party: 1 Text: HAPPENING NOW: Join us for a live telephone town hall discussing issues related to:\n",
      "🦠 #COVID19\n",
      "👪 Keeping your loved ones safe\n",
      "📝 Small business resources pertaining to COVID-19\n",
      "🇺🇸 #CARESAct\n",
      "🆘 Constituent resources related to COVID-19\n",
      "\n",
      "☎️ CALL: 540-409-4375; Ext.57946# https://t.co/jyLhdVpPVV\n",
      "Party: 0 Text: We just can’t just govern on a wish and a prayer. We need the Administration to work with us to navigate out of this mess.\n",
      "https://t.co/HJQX657p1l\n",
      "Party: 0 Text: Stopped at LD’s in East Troy for lunch! https://t.co/VQGmclU7KY\n",
      "Party: 1 Text: I have worked with our counties to request additional testing materials as the State continues to make the decisions about where tests are deployed. The current testing percentage in the North Country is unacceptable and needs to be addressed immediately.\n",
      "Party: 1 Text: @MarioDB @CDCgov .@MarioDB Hey Friend, \n",
      "Many prayers coming your way for a speedy recovery!!\n",
      "Party: 0 Text: Special interest groups should not be allowed to derail this meaningful bipartisan action on energy efficiency in Congress. These are *voluntary* building codes that will help reduce emissions &amp; energy use while creating clean energy jobs. The obstruction needs to end! https://t.co/YQM5yYcLH6\n",
      "Party: 1 Text: It's #LawEnforcementWeek, and I'm so grateful for the men and women who keep our communities safe. To those who wear the badge and to those who have made the ultimate sacrifice in the line of duty, thank you. https://t.co/YLoQhLhfjm\n",
      "Party: 1 Text: 📣Businesses in Lockhart impacted by the #COVID19 pandemic can apply for one-time grant assistance up to $2,500. Learn more: https://t.co/tZCSqrdgkr\n",
      "Party: 1 Text: More info from @NewsAdVantage ⬇️\n",
      "https://t.co/2CDj4fTYxK\n",
      "Party: 1 Text: Yesterday, the House passed the SHIELD for Veterans Act, which included a common-sense provision that I introduced to require the VA to allow veterans to make this change online. (2/2) https://t.co/uzV9lp41gM\n",
      "Party: 1 Text: Our goal is to keep employees connected to employers during this crisis. \n",
      "\n",
      "Get the facts about the Paycheck Protection Program &amp; the Employee Retention Tax Credit that your business may be eligible for ⬇️\n",
      "https://t.co/ZYVVrzHob5\n",
      "Party: 0 Text: As I stated at a @FinancialCmte hearing last month, Democrats who are upset with the current #CFPB should work w/ Republicans to revise the structure. Creating a board and making CFPB subject to appropriations so Congress has a say on its actions will bring accountability to CFPB\n",
      "Party: 0 Text: Translation: he doesn’t give a care about the financial future of the 60K American workers of @goodyear, unless he can sell his #MadeInChina ugly hat! #AmericasGreatestMistake https://t.co/OaFVvO2u0d\n",
      "Party: 1 Text: This #MLKday, we honor Dr. King’s tireless work for freedom and equality in our great nation and celebrate his legacy in the Civil Rights movement. 🇺🇸\n",
      "Party: 1 Text: Happy 245th Birthday to the @USNavy! I’m grateful for the men and women in uniform who serve to defend our freedoms and protect our country at home and abroad. #245NavyBday https://t.co/OEeqMsuSj2\n",
      "Party: 0 Text: Montana’s educators are the best in the business on any given day—but their ability to keep students engaged and adapt to distance learning has shown their ingenuity and dedication to serving our next generation of leaders. Happy #TeacherAppreciationDay to MT teachers everywhere. https://t.co/JJKW09tABa\n",
      "Party: 1 Text: The last thing families need is a surprise medical bill showing up in the mail months after receiving care. \n",
      "\n",
      "Check out Mike’s latest ad to hear what he’s doing to solve it. \n",
      "\n",
      "https://t.co/THxPOS8o1A https://t.co/URdXiyqArj\n",
      "Party: 1 Text: Charter is playing a vital role in connecting rural Montana with broadband internet access. https://t.co/txqlooXES4\n",
      "Party: 0 Text: @TimCaravella Thanks for having me! Really enjoyed learning about your business story and the work you do.\n",
      "Party: 1 Text: Even if you feel healthy we must use social distancing to slow the spread of the virus.  A spike in COVID-19 cases could overwhelm our hospitals. We all have a part to play to #flattenthecurve https://t.co/IH23oky4rJ\n",
      "Party: 1 Text: Angela’s brother served with distinction as a law enforcement officer until he was murdered, anonymously, during the riots in Oakland. He took his last breath on the cement courthouse steps after being shot multiple times.\n",
      "\n",
      "I hope everyone watches her powerful testimony. https://t.co/PCfUXeBsBG\n",
      "Party: 0 Text: I have put together a page on my website to centralize information about how we are responding to this crisis and get you to the resources that will help you. Be sure to visit my website and know you can always call my office if you need help right away. https://t.co/fRYOtIDi9o\n",
      "Party: 0 Text: It's a disgrace that multi-billion dollar businesses are receiving #COVID19 payouts while hardworking Americans can't get the unemployment benefits they've earned!\n",
      "https://t.co/NmuPx0aFwl\n",
      "Party: 1 Text: Lea mi opinión en @elnuevoherald sobre las dictaduras en nuestro hemisferio y la necesidad de mantener una política exterior consistente hacia ellas. 👇 https://t.co/ldfT3yPFBh\n",
      "Party: 1 Text: To help the North Country prepare to reopen and meet the state and local mandates, @NCountryChamber has created this list of resources to navigate state and local requirements under Phase 1 of reopening. https://t.co/ZDVuEBs0nK\n",
      "Party: 1 Text: Thank you to all the reporters who have stuck with us through long markups year after year.  We are in the home stretch, and the RipIts are available in the hearing room!\n",
      "Party: 1 Text: Chanting “Stop the Count” will not: unelect Joe Biden, halt COVID’s deadly spread, or end the countdown to Trump‘s departure in 65 days.\n",
      "Party: 1 Text: Interesting read from @SecBrouillette. \"Coal is also crucial to our economic security. A strong coal future will power our manufacturing base, preserving American jobs and creating new employment and economic opportunity across the country.\" \n",
      "Read more. ⬇️\n",
      "https://t.co/mzDBCDOSeM\n",
      "Party: 1 Text: The @austintexasgov just launched an online #COVID19 assessment test.\n",
      "\n",
      "Check it out here 👇🏻 \n",
      "\n",
      "https://t.co/mXwAtfQGjh\n",
      "Party: 1 Text: Currently, there are over 111,000 confirmed cases of #COVID19 worldwide, with over 500 confirmed cases in the United States. \n",
      "\n",
      "If you plan to travel, please read @StateDept’s country-by-country travel advisory for additional up-to-date information: https://t.co/8xDJOJQhqS\n",
      "Party: 1 Text: .@UofLHealth CEO Tom Miller: \"This CARES Act funding is an essential backstop to the backbone of our health care system. We applaud the leadership of @SenateMajLdr McConnell to ensure our safety net remains strong.” https://t.co/vMHoSk0MXC\n",
      "Party: 1 Text: On the Floor to urge my colleagues to support the HEALTH Act a bipartisan effort I introduced to help increase access to local #telehealth The bill will codify #Medicare reimbursement for community health centers &amp; rural health clinics for #telemedicine\n",
      "Party: 1 Text: 2) Expanded SBA loan authority. \n",
      "\n",
      "We need money in the hands of those employing millions of Americans so they can bridge the gap. It does no good to mandate “paid leave” from a job that doesn’t exist because the business went under. Let’s be smart about it.\n",
      "Party: 1 Text: An investigation I spearheaded with @putkids1st found Texas is failing our youngest learners with disabilities because of years of insufficient state funding.\n",
      "\n",
      "Infants + toddlers must have access to early inventions to ensure they can be lifelong learners.\n",
      "https://t.co/SF2o8zvIB1\n",
      "Party: 1 Text: Stellate ganglion block (SGB) is proven to alleviate common PTSD symptoms, including hyperarousal and anxiety, by injecting an anesthetic agent onto a collection of nerves in the neck responsible for the activation of the \"fight or flight\" response.\n",
      "Party: 1 Text: (11/21) https://t.co/0I7KHVal1T\n",
      "Party: 1 Text: The PRO Act uses Hoffa-era union intimidation tactics that effectively increase campaign donations to Democrats. \n",
      "\n",
      "This bill is the Union Boss Wish List, overturning state right-to-work laws and forcing employers to handover private worker information to union organizers. https://t.co/gA970MYwuN\n",
      "Party: 1 Text: Two thirds of college students want to return to campus, according to an @Axios survey. At @LifeAtPurdue, tuition deposits by incoming freshmen broke last year’s record.\n",
      "Party: 1 Text: Today I spoke in support of H.R. 2382, the USPS Fairness Act. I am proud to join my colleagues in advancing this legislation to the Senate. This bill will allow the postal service to devote resources to critical services its employees deliver to our community. https://t.co/5xGrww7gUQ\n",
      "Party: 1 Text: Meanwhile she’s actively delaying negotiations for another COVID19 relief package that would help struggling Americans and first responders... https://t.co/PQvYqIqyTp\n",
      "Party: 1 Text: Joined @USDA Secretary Sonny Perdue &amp; Senator Cornyn today to speak to #Texas #farmers &amp; #ranchers about the impact #COVID19 has had on their operations. \n",
      "\n",
      "I’m committed to fighting for Central Texans’ needs as we negotiate another coronavirus response bill in Congress. https://t.co/aGbhtMyEyu\n",
      "Party: 1 Text: Christy and I are devastated and heartbroken to hear of the sudden passing of Carol Barr. Our hearts and our prayers are with @RepAndyBarr and his two young daughters during this difficult time.\n",
      "Party: 0 Text: As Missouri surpassed 100K cases over the weekend, testing remains crucial to combating #COVID19. @JacksonCountyHD is hosting FREE tests for the community. Asymptomatic individuals are encouraged to call 404-CARE to schedule a test at one of @TrumanMedKC hospital locations. https://t.co/OomYtI8Hol\n",
      "Party: 0 Text: Up-to-date health care guidance from @CDCgov &amp; @AZDHS l\n",
      "➡️ https://t.co/HZWAXRkGxE\n",
      "Party: 1 Text: Great news that is going under-reported → the Trump Administration already has contracts guaranteeing the production of millions of vials of potential successful vaccine candidates—some with a deadline as early as this year. https://t.co/otQ4cjI9bO\n",
      "Party: 1 Text: If Dems are so confident, why are places like DC and NYC boarding up?\n",
      "\n",
      "Is a hurricane approaching?\n",
      "Party: 0 Text: I hope you can join us TONIGHT for one of my last virtual receptions before the election! https://t.co/BneWZGia9B\n",
      "Party: 0 Text: As National Suicide Prevention Week comes to a close, it is important to remember there are several resources available to #KeepGoing. You can always call the National Suicide Prevention Lifeline at 1-800-273-TALK (8255) or contact the Crisis Text Line by texting TALK to 741741. https://t.co/H1egSG2JwP\n",
      "Party: 1 Text: GREAT NEWS!! The U.S. unemployment rate DROPPED in May. The Labor Department reported this morning that employers ADDED 2.5 million jobs last month. The unemployment rate stands at 13.3%, but economists had predicted the report would show 19.5%.\n",
      "Party: 1 Text: ALERT: @GovernorVA today shared a new set of online tools to help Virginians take full advantage of the supportive services available through the Commonwealth’s workforce system. For more information, click here https://t.co/hN6iEYkwGW and here https://t.co/VtS4WBqUQx.\n",
      "Party: 1 Text: My bill, the CABLE Competition Act, will lead to more consumer choice and more competitive prices: https://t.co/ZkJPl3fTlJ\n",
      "Party: 1 Text: Treasury Dept circulating fact sheet that provides, as to direct assistance, “Payment amounts would be fixed and tiered based on income level and family size.” This is the right approach\n",
      "Party: 1 Text: Our top priority must be maintaining law &amp; order and keeping our communities safe. Congressman Tiffany will oppose efforts to defund our police departments, and focus instead on increasing funding for better police training and equipment such as body cameras. -staff\n",
      "Party: 1 Text: I believe her. https://t.co/CSgwa4tCQs\n",
      "Party: 1 Text: Speaker Pelosi - you made a huge tactical error by ripping up the speech - thumbing your nose at millions of Americans who are better off today (as shown in the stats) and magnified by the POTUS’ SOTU tonight. America saw your pettiness. #HopeYouNoticedAmerica\n",
      "Party: 1 Text: @POTUS: \"To safeguard American Liberty, we have invested a record-breaking $2.2 trillion in the United States Military.  We have purchased the finest planes, missiles, rockets, ships, and every other form of military equipment -- all made in the United States of America.\" #SOTU\n",
      "Party: 1 Text: In the wake of #GeorgeFloyd’s death, our anger and grief is justified. People have a right to protest peacefully. But what we’ve seen in Atlanta is out of character for our great city and our great state.\n",
      "\n",
      "https://t.co/R2ul66x0qW\n",
      "Party: 1 Text: With so much uncertainty and concern surrounding COVID-19, it seems like good news has become harder to find. Well, actor @johnkrasinski has started the @somegoodnews broadcast from his home to remind us of many of the silver linings in the world today. https://t.co/0OHfuljPDI\n",
      "Party: 1 Text: We have more evidence of corruption about @JoeBiden than *ever* existed about @realDonaldTrump.\n",
      "\n",
      "But all the loud voices in media and government who called for aggressive investigations of @POTUS are now silent or busy denouncing the evidence.\n",
      "\n",
      "These people have zero credibility.\n",
      "Party: 1 Text: https://t.co/RGpzRG6iyk https://t.co/6UplBwP9AN\n",
      "Party: 1 Text: Our response to #Covid_19 needs to be taken seriously. We have to put country and public safety above politics so that Congress can properly help the American people through this crisis.\n",
      "https://t.co/b1aUD9zxDV\n",
      "Party: 1 Text: The rule changes to H-1B visas are much needed reforms to a wildly abused program.  \n",
      "\n",
      "American workers who are struggling to get back on their feet will benefit from closing these loopholes.\n",
      "\n",
      "https://t.co/J9LCvrogRY\n"
     ]
    }
   ],
   "source": [
    "#the text of some incorrectly classified tweets. This is mostly to see if those incorrectly classified are \"hard\" to classify (that is, could a person identify them)\n",
    "\n",
    "majority_wrong = rnn_df_test_copy[majority_predictions!=y_test]\n",
    "print(majority_wrong)\n",
    "wrong_sample = majority_wrong.sample(n=100)\n",
    "for index, row in wrong_sample.iterrows():\n",
    "    print('Party: %s Text:' % row['party'], row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum scores of test examples predicted wrongly by majority\n",
      "# with score 0:\n",
      "7707\n",
      "# with score 1:\n",
      "7736\n",
      "# with score 2:\n",
      "4143\n",
      "# with score 3:\n",
      "35\n",
      "percent with score 1 or 2 (so percent with disagreement):\n",
      "73.65908104421158\n"
     ]
    }
   ],
   "source": [
    "#wanted to determine the scores of the wrongly predicted. If all the models predict incorrectly those the majority predicts incorrectly,\n",
    "# then it's unlikely ensemble learning is helpful for this classification task\n",
    "\n",
    "wrong_sum_predictions =sum_predictions[majority_predictions!=y_test]\n",
    "\n",
    "print(\"sum scores of test examples predicted wrongly by majority\")\n",
    "print(\"# with score 0:\")\n",
    "print(sum(wrong_sum_predictions==0))\n",
    "print(\"# with score 1:\")\n",
    "print(sum(wrong_sum_predictions==1))\n",
    "print(\"# with score 2:\")\n",
    "print(sum(wrong_sum_predictions==2))\n",
    "print(\"# with score 3:\")\n",
    "print(sum(wrong_sum_predictions==3))\n",
    "print(\"percent with score 1 or 2 (so percent with disagreement):\")\n",
    "print((sum(wrong_sum_predictions==1) + sum(wrong_sum_predictions==2))/sum(wrong_sum_predictions)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum scores of test examples predicted wrongly by majority\n",
      "# with score 0:\n",
      "7707\n",
      "# with score 1:\n",
      "7736\n",
      "# with score 2:\n",
      "4143\n",
      "percent with score 1 (so percent with disagreement):\n",
      "47.96924412475972\n"
     ]
    }
   ],
   "source": [
    "#The same analysis above, but ignoring the random forest since it just predicts Democrat for almost every example\n",
    "\n",
    "lr_rnn_sum_predictions =  np.add(lr_predictions, rnn_predictions)\n",
    "\n",
    "lr_rnn_wrong_sum_predictions =sum_predictions[majority_predictions!=y_test]\n",
    "\n",
    "print(\"sum scores of test examples predicted wrongly by majority\")\n",
    "print(\"# with score 0:\")\n",
    "print(sum(lr_rnn_wrong_sum_predictions==0))\n",
    "print(\"# with score 1:\")\n",
    "print(sum(lr_rnn_wrong_sum_predictions==1))\n",
    "print(\"# with score 2:\")\n",
    "print(sum(lr_rnn_wrong_sum_predictions==2))\n",
    "print(\"percent with score 1 (so percent with disagreement):\")\n",
    "print((sum(wrong_sum_predictions==1))/sum(wrong_sum_predictions)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Republican of those misclassified by the majority vote\n",
      "78.70648794658784\n"
     ]
    }
   ],
   "source": [
    "# breaking down the wrong by party\n",
    "\n",
    "print('Percent Republican of those misclassified by the majority vote')\n",
    "print(sum(majority_wrong['party']) / len(majority_wrong['party'])*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
